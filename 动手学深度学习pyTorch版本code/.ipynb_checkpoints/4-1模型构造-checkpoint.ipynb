{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里重载了Module类的__init__函数和forward函数。\n",
    "它们分别用于创建模型参数和定义前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP类中无需定义backward，系统通过autograd自动生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[0.3606, 0.1625, 0.5198,  ..., 0.1402, 0.0229, 0.1755],\n",
      "        [0.8274, 0.3224, 0.3918,  ..., 0.4560, 0.2947, 0.1576]])\n",
      "tensor([[ 0.1872, -0.0285, -0.0048,  0.1915, -0.0012,  0.2103,  0.0831, -0.1248,\n",
      "          0.0165, -0.1123],\n",
      "        [ 0.2310,  0.1230, -0.0301, -0.0125,  0.0896,  0.0670,  0.0664,  0.0432,\n",
      "         -0.0810,  0.1020]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 实例化\n",
    "X = torch.rand(2,784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "print(X)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module的子类\n",
    "Sequential、\n",
    "ModuleList、\n",
    "ModuleDict、\n",
    "等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential类\n",
    "它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加Module的实例，\n",
    "而模型的前向计算就是将这些实例按添加的顺序逐一计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self,*args):\n",
    "        super(MySequential,self).__init__()\n",
    "        # 如果传入的是一个OrderedDict\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict):\n",
    "            for key,module in args[0].items():\n",
    "                self.add_module(key,module) ## add_module方法会将module添加进self._modules(一个OrderedDict)\n",
    "        else: # 传入的是一些Module\n",
    "            for idx,module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成员\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[ 0.2821, -0.1592,  0.0299, -0.1609,  0.0183,  0.0142, -0.0844,  0.0586,\n",
      "         -0.1750, -0.0607],\n",
      "        [ 0.2305, -0.0590,  0.0634, -0.2694,  0.0774,  0.2081, -0.1154, -0.0249,\n",
      "         -0.1559, -0.0275]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = MySequential(\n",
    "    nn.Linear(784,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10),\n",
    ")\n",
    "print(net)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModuleList类\n",
    "ModuleList接收一个子模块的列表作为初始化参数，\n",
    "最主要的特点是可以类似List那样进行**append**和**extend**操作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "net.append(nn.Linear(256, 10)) # # 类似List的append操作\n",
    "print(net[-1])  # 类似List的索引访问\n",
    "print(net)\n",
    "\n",
    "\n",
    "# ModuleList实例没有实现forward，会报NotImplementedError\n",
    "net(torch.zeros(1, 784)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ModuleList的实际作用**\n",
    "ModuleList实例仅仅是一个存储各种模块的列表，没有实现模块的**forward**功能需要自己实现。\n",
    "\n",
    "ModuleList的出现只是让网络定义前向传播更加方便。（减少代码量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官网的例子\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x) # //表示整数除法\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2283,  0.5613, -0.5006, -0.9177, -0.1075,  0.0239,  0.1398,  0.5800,\n",
       "         -0.1388,  0.2570]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MyModule()\n",
    "net(torch.zeros(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ModuleList的特点**\n",
    "加入到ModuleList的所有模块的参数会自动添加到整个网络中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1:\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "net2:\n"
     ]
    }
   ],
   "source": [
    "class Module_ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_ModuleList, self).__init__()\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(10, 10),\n",
    "        ])\n",
    "\n",
    "class Module_List(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_List, self).__init__()\n",
    "        self.linears = [nn.Linear(10, 10)]\n",
    "\n",
    "net1 = Module_ModuleList()\n",
    "net2 = Module_List()\n",
    "\n",
    "print(\"net1:\")\n",
    "for p in net1.parameters():\n",
    "    print(p.size())\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print(\"net2:\")\n",
    "for p in net2.parameters():\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModuleDict\n",
    "接受一个子模块的字典作为初始化参数。可以像字典那样访问子模块。\n",
    "\n",
    "同样，ModuleDict类的实例也需要手动实现forward。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleDict(\n",
      "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleDict({\n",
    "    'linear': nn.Linear(784, 256),\n",
    "    'act': nn.ReLU(),\n",
    "})\n",
    "net['output'] = nn.Linear(256, 10) # 添加\n",
    "print(net['linear']) # 访问\n",
    "print(net.output)\n",
    "print(net)\n",
    "\n",
    "# net是ModuleDict实例，因为没有实现forward因此进行前向传播会报错\n",
    "net(torch.zeros(1, 784)) # 会报NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1168, 0.2491, 0.2643, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1954,\n",
      "         0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 实现net是ModuleDict实例的forward函数\n",
    "class MyModuleDict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModuleDict, self).__init__()\n",
    "        self.nets = nn.ModuleDict({\n",
    "                            'linear': nn.Linear(10, 10),\n",
    "                            'act': nn.ReLU(),\n",
    "                        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nets['linear'](x)\n",
    "        o = self.nets['act'](x)\n",
    "        return o\n",
    "\n",
    "MyModuleDictNet = MyModuleDict()\n",
    "print(MyModuleDictNet(torch.zeros(1, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造复杂的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(FancyMLP,self).__init__(**kwargs)\n",
    "        \n",
    "        # 不可训练参数\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=False)\n",
    "        \n",
    "        self.linear = nn.Linear(20,20)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # 使用创建的常数参数，以及nn.functional中的relu函数和mm函数\n",
    "        x = nn.functional.relu(torch.mm(x,self.rand_weight.data) + 1)\n",
    "        \n",
    "        # 复用全连接层，等价于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # 控制流，这里我们需要调用item函数来返回标量进行比较\n",
    "        while x.norm().item() > 1:\n",
    "            x /=  2\n",
    "        if x.norm().item() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FancyMLP(\n",
      "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      ")\n",
      "tensor(-0.4770, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones(2,20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "1、与Sequential不同，ModuleList和ModuleDict并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义forward函数。\n",
    "\n",
    "2、可以直接使用Sequential来构造模型，也可以选择继承Module来构造模型。后者可以构造出很复杂的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
