{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences: 42068'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 'ptb.train.txt' in os.listdir(\"data\")\n",
    "\n",
    "with open('data/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    # st是sentence的缩写\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "\n",
    "'# sentences: %d' % len(raw_dataset) # 输出 '# sentences: 42068'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 15,\n",
      "\n",
      " ['pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N']\n",
      "# tokens: 11,\n",
      "\n",
      " ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group']\n",
      "# tokens: 23,\n",
      "\n",
      " ['rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[1:4]:\n",
    "    print('# tokens: %d,\\n\\n' % len(st),st[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立词语索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了计算简单，只保留在数据集中至少出现5次的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tk是token的缩写\n",
    "# Counter是自动计数器，将产生一个字典(词:词的出现次数)\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将词映射到整数索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 15,\n",
      "\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2]\n",
      "# tokens: 11,\n",
      "\n",
      " [14, 1, 15, 16, 17, 1, 18, 7, 19, 20, 21]\n",
      "# tokens: 23,\n",
      "\n",
      " [22, 1, 2, 3, 4, 23, 24, 16, 17, 25, 26, 27, 28, 29, 30, 10, 11, 12, 17, 31, 32, 33, 34]\n"
     ]
    }
   ],
   "source": [
    "# 词->整数\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "# 整数->词\n",
    "token_to_idx = {tk: idx for idx,tk in enumerate(idx_to_token)}\n",
    "# 将每个句子的词翻译成整数索引，一个句子就是一个整数数组\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "          for st in raw_dataset]\n",
    "# 总的词数\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "'# tokens: %d' % num_tokens # 输出 '# tokens: 887100'\n",
    "\n",
    "for st in dataset[1:4]:\n",
    "    print('# tokens: %d,\\n\\n' % len(st),st[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二次采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常来说，在一个背景窗口中，一个词与较低频词同时出现比和较高频词同时出现对训练词嵌入模型更有益，\n",
    "\n",
    "因此训练词嵌入模型时，可以对词进行二次采样。\n",
    "\n",
    "数据集中，每个被索引词wi将有一定概率被丢弃，丢弃概率：\n",
    "\n",
    "$$ P\\left(w_{i}\\right)=\\max \\left(1-\\sqrt{\\frac{t}{f\\left(w_{i}\\right)}}, 0\\right) $$\n",
    "\n",
    "1. t是一个超参数，一般取10E-4\n",
    "2. f(w)表示数据集中词w的个数占总词数之比，可见f(w) > t时，词w才有可能被丢弃。频率越大，丢弃概率越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 375715'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    # 0到1均匀地取随机数，则有P(w)的概率取值小于P(w)\n",
    "    # 如果随机数取到这个区间，则代表该词需要丢弃\n",
    "    return random.uniform(0,1) < 1- math.sqrt(\n",
    "        1e-4 / (counter[idx_to_token[idx]] / num_tokens)\n",
    "    )\n",
    "\n",
    "# 遍历数据集的所有句子st，\n",
    "# 遍历每个句子的所有词tk\n",
    "subsampled_dataset = [[tk for tk in st if not (discard(tk))] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset]) # '# tokens: 375875'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二次采样后，我们去掉了一半左右的词，高频词the的采样率不足1/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# the: before=50770, after=2055'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (\n",
    "        token, \n",
    "        sum([st.count(token_to_idx[token]) for st in dataset]), \n",
    "        sum([st.count(token_to_idx[token]) for st in subsampled_dataset])\n",
    "    )\n",
    "\n",
    "compare_counts('the') # '# the: before=50770, after=2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取中心词和背景词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "背景词：与中心词距离不超过背景窗口大小的词作为它的背景词。\n",
    "\n",
    "以下函数提取所有中心词及其背景词，其中背景窗口大小在[1,max_window_size]之间随机选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所有中心词及其背景词，背景窗口大小在[1,max_window_size]之间随机选取\n",
    "# dataset:多个以索引表示的句子\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2: # 每个句子至少要有2个词才可能组成一对\"中心词-背景词\"\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            # 每个句子对应的最大窗口是随机的\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            # 背景词索引\n",
    "            indices = list(range(\n",
    "                max(0, center_i - window_size),\n",
    "                min(center_i + 1 + window_size, len(st))\n",
    "            ))\n",
    "            indices.remove(center_i) # 将中心词排除在背景词之外\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers,contexts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们创建一个人工数据集，其中含有词数分别为7和3的两个句子。\n",
    "\n",
    "设最大背景窗口为2，打印所有中心词和它们的背景词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [2, 4]\n",
      "center 4 has contexts [2, 3, 5, 6]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8, 9]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验中，我们令最大窗口数=5，下面提取数据集中所有的中心词和背景词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 负采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用负采样来进行**近似训练**，对于一对中心词和背景词，我们随机采样K个噪声词（实验中K=5），\n",
    "\n",
    "根据论文建议，**噪声词采样概率P(w)** 设为w词频与总词频之比的0.75次方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [],[],0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        # 为什么随机采样 len(contexts) * K 个噪声词？\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            \n",
    "            if i == len(neg_candidates):\n",
    "                # 根据每个词的权重(sampling_weights)随机生成k个词的索引作为噪声词\n",
    "                # 为了高效计算，将k设得稍大一些\n",
    "                # 如果i使用到了1e5，则再随机生成k个噪声词\n",
    "                i , neg_candidates = 0, random.choices(\n",
    "                    # 从population中选取k次，不同元素被选取的相对权重由sampling_weights定义\n",
    "                    population, sampling_weights, k = int(1e5) \n",
    "                )\n",
    "                \n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            \n",
    "            # 噪声词不能是背景词\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "                \n",
    "        all_negatives.append(negatives)\n",
    "        \n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样权重\n",
    "sampling_weights = [(counter[w]/ num_tokens)**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(all_contexts[0]))\n",
    "print(len(all_negatives[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random.choices：\n",
    "1. population：集群。\n",
    "2. weights：相对权重。\n",
    "3. cum_weights：累加权重。\n",
    "4. k：选取次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 4, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "w = [1,0,0,1,1] # 相对权重\n",
    "res = random.choices(a,w,k=5)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据集中提取所有中心词all_centers，以及每个中心词对应的背景词all_contexts和噪声词all_negatives。\n",
    "\n",
    "这里，我们自定义一个Dataset类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个样本包括**一个中心词**和对应的**n个背景词**和**m个噪声词**。\n",
    "\n",
    "由于每个样本选取的背景窗口大小是随机的，所以各样本n+m也会不同。\n",
    "\n",
    "在构造小批量时，将每个样本的背景词和噪声词连接在一起，并**填充0项**以使长度相同。即长度为最大的样本的m+n。\n",
    "\n",
    "为了避免填充项对损失函数计算的影响，我们构建**掩码变量masks**，该掩码变量每个元素与连接后的背景词噪声词contexts_negatives中的元素一一对应。如果mask对应到填充项，则masks中相同位置的元素取0.否则取1.\n",
    "\n",
    "为了区分正类和负类，我们还需要将contexts_negatives变量中的背景词和噪声词区分开：具体思路是创建与contexts_negatives形状相同的**标签变量labels**，并将背景词(正类)对应的元素设1，其余设0.\n",
    "\n",
    "下面我们实现这个小批量读取函数batchify："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入data：是一个长度为批量大小的列表，其中每个元素分别包含中心词center、背景词context和噪声词negative\n",
    "def batchify(data):\n",
    "    \"\"\"\n",
    "    用作DataLoader的参数collate_fn: 输入是个长为batchsize的list, \n",
    "    list中的每个元素都是Dataset类调用__getitem__得到的结果\n",
    "    \"\"\"\n",
    "    max_len = max(len(c) + len(n) for _,c,n in data)\n",
    "    centers, contexts_negatives,masks,labels = [],[],[],[]\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len-cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len-cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (\n",
    "        torch.tensor(centers).view(-1,1), # 列向量\n",
    "        torch.tensor(contexts_negatives),\n",
    "        torch.tensor(masks),\n",
    "        torch.tensor(labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用刚刚定义的batchify函数指定DataLoader实例中小批量的读取方式，然后打印读取的第一个批量中各个变量的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 0\n",
    "\n",
    "dataset = MyDataset(all_centers,all_contexts,all_negatives)\n",
    "data_iter = Data.DataLoader(dataset,batch_size,shuffle=True,\n",
    "                            collate_fn=batchify,# 自定义DataLoader的小批量读取方式\n",
    "                            num_workers = num_workers\n",
    "                           )\n",
    "\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name,'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 跳字模型\n",
    "我们将通过使用**嵌入层**和**小批量乘法**来实现跳字模型。它们也常常用于实现其他自然语言处理的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入层\n",
    "获取词嵌入的层称为嵌入层，可以通过创建nn.Embedding实例得到。\n",
    "嵌入层的**权重**是一个矩阵\n",
    "1. 行数为词典大小（num_embeddings）\n",
    "2. 列数为每个词向量的维度（embedding_dim）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 4])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们设词典大小为20，词向量维度为4：\n",
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "embed.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入层的**输入**是词的索引。\n",
    "\n",
    "输入一个词的索引i，嵌入层**返回**权重矩阵的第i行作为它的词向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[[ 0.8220, -0.1172,  0.2035, -3.2670],\n",
      "         [-0.1323,  0.7140, -0.6359, -0.7604],\n",
      "         [-0.0939, -1.1068,  1.3087,  0.0738]],\n",
      "\n",
      "        [[-0.5157,  0.2548, -0.0407,  1.9367],\n",
      "         [ 1.9984,  0.4180,  0.1604, -1.0349],\n",
      "         [ 0.6693, -1.7447, -0.4789, -0.1335]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]], dtype = torch.long)\n",
    "print(x.shape) # x:(2,3)\n",
    "# 返回6个词的词向量，每个词向量为4维\n",
    "res = embed(x)\n",
    "print(res.shape)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小批量乘法\n",
    "给定两个形状分别为(n,a,b)和(n,b,c)的Tensor，小批量乘法输出的形状为(n,a,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 6])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones((2,1,4)) # 两个1,4的矩阵\n",
    "Y = torch.ones((2,4,6)) # 两个4,6的矩阵\n",
    "print(torch.bmm(X,Y).shape)\n",
    "\n",
    "X = torch.ones((2,2,3)) # 两个1,4的矩阵\n",
    "Y = torch.ones((2,2,3)) # 两个4,6的矩阵\n",
    "print(torch.bmm(X,Y.permute(0, 2, 1)).shape) # 替换维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跳字模型的前向计算\n",
    "前向计算中，跳字模型的输入包含\n",
    "1. 中心词索引center：(batch_size,1)\n",
    "2. 连接的背景词和噪声词索引：contexts_and_negatives：(batch_size,max_len)\n",
    "\n",
    "这两个变量分别通过词嵌入层由词索引变为词向量，维度分别为：\n",
    "1. (batch_size,1,embedding_dim)\n",
    "2. (batch_size,max_len,embedding_dim)\n",
    "\n",
    "再通过小批量乘法得到形状为(batch_size,1,max_len)的输出，\n",
    "\n",
    "**输出中每个元素是中心词向量与背景词向量或噪声词向量的内积。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向计算\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    # v:(batch_size,1,embedding_dim)\n",
    "    v = embed_v(center)\n",
    "    \n",
    "    # u:(batch_size,max_len,embedding_dim)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    \n",
    "    # 如何理解perd的含义？\n",
    "    pred = torch.bmm(v,u.permute(0,2,1)) # permute将维度换位，即dim=2与dim=1维度互换，dim=1与dim2维度互换\n",
    "    \n",
    "    return pred # (batch_size,1,max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permute的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "torch.Size([2, 1, 3])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]])\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1,2,3],[4,5,6]]])\n",
    "unpermuted = torch.tensor(a)\n",
    "print(unpermuted.size())\n",
    "print(unpermuted)\n",
    "permuted = unpermuted.permute(1,0,2)\n",
    "print(permuted.size())\n",
    "print(permuted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 60])\n",
      "torch.Size([10, 60])\n"
     ]
    }
   ],
   "source": [
    "center_ = torch.tensor(np.ones((10,1,4)))\n",
    "cotext_and_negative_ = torch.tensor(np.ones((10,60,4)))\n",
    "res = torch.bmm(center_,cotext_and_negative_.permute(0,2,1))\n",
    "print(res.shape)\n",
    "print(res.view((10,60)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二元交叉熵损失函数\n",
    "二元交叉熵损失函数：https://zhuanlan.zhihu.com/p/326691760\n",
    "\n",
    "根据负采样中损失函数的定义，我们可以使用二元交叉熵损失函数,下面定义SigmoidBinaryCrossEntropyLoss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self): # none mean sum\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        \"\"\"\n",
    "        input - Tensor shape:(batch_size, len)\n",
    "        target - Tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\",weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "    \n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过掩码变量指定小批量中参与损失函数计算的部分预测值和标签：\n",
    "1. 当掩码为1时，相应位置的预测值和标签将参与损失函数的计算；\n",
    "2. 当掩码为0时，相应位置的预测值和标签则不参与损失函数的计算。\n",
    "\n",
    "我们之前提到，掩码变量可用于避免填充项对损失函数计算的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred.shape: torch.Size([2, 4])\n",
      "mask.shape[1]: 4\n",
      "mask.float().sum(dim=1): tensor([4., 3.])\n",
      "loss(pred, label, mask): tensor([0.8740, 0.9075])\n",
      "loss: tensor([0.8740, 1.2100])\n"
     ]
    }
   ],
   "source": [
    "# 假设有两个样本及其对应的标签和掩码\n",
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "# 标签变量label中的1和0分别代表背景词和噪声词\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # 掩码变量\n",
    "print('pred.shape:',pred.shape)\n",
    "print('mask.shape[1]:',mask.shape[1])\n",
    "print('mask.float().sum(dim=1):',mask.float().sum(dim=1))\n",
    "print('loss(pred, label, mask):',loss(pred, label, mask))\n",
    "\n",
    "print('loss:',loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为比较，下面将从零开始实现二元交叉熵损失函数的计算，并根据掩码变量mask计算掩码为1的预测值和标签的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return - math.log(1 / (1 + math.exp(-x))) \n",
    "\n",
    "# 注意1-sigmoid(x) = sigmoid(-x)\n",
    "print('%.4f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4)) \n",
    "print('%.4f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100 # 词向量维度100\n",
    "net = nn.Sequential(\n",
    "    # 权重的行数=词典大小\n",
    "    # 权重的列数=词向量维度\n",
    "    # 权重的值其实就是我们想得到的最终训练结果\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练函数\n",
    "这个训练函数的目的是net[0]和net[1]的权重，权重也就是词向量。\n",
    "\n",
    "因此训练完模型后，就得到了词向量。\n",
    "\n",
    "考虑到填充项的存在，损失函数有些不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,lr,num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on:\",device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            \n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            \n",
    "            # 使用掩码变量mask来避免填充项对损失函数计算的影响\n",
    "            # pred:(batch_size,1,max_len) ，预测输出\n",
    "            # label:(batch_size,max_len) ，标签\n",
    "            l = (loss( pred.view(label.shape), label, mask ) * \n",
    "                mask.shape[1] / mask.float().sum(dim=1)).mean() # 一个batch的平均loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "            \n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on: cpu\n",
      "epoch 1, loss 1.96, time 59.08s\n",
      "epoch 2, loss 0.62, time 59.27s\n",
      "epoch 3, loss 0.45, time 57.38s\n",
      "epoch 4, loss 0.40, time 58.73s\n",
      "epoch 5, loss 0.37, time 63.03s\n",
      "epoch 6, loss 0.35, time 56.66s\n",
      "epoch 7, loss 0.34, time 56.54s\n",
      "epoch 8, loss 0.33, time 57.73s\n",
      "epoch 9, loss 0.32, time 60.29s\n",
      "epoch 10, loss 0.32, time 59.28s\n"
     ]
    }
   ],
   "source": [
    "# 学习率：0.01，周期：10\n",
    "train(net, 0.01, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用词嵌入模型\n",
    "训练好词嵌入模型之后，我们就借助词向量计算词与词之间的相似度。\n",
    "具体做法是计算两个词向量的**余弦相似度**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9858, 100])\n",
      "torch.Size([100])\n",
      "cosine sim=0.327: reruns\n",
      "cosine sim=0.322: renamed\n",
      "cosine sim=0.321: hit\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    print(W.shape)\n",
    "    x = W[token_to_idx[query_token]] # weight的其中一行为query_token的词向量\n",
    "    print(x.shape)\n",
    "    # 添加的1e-9是为了数值稳定性\n",
    "    # cos：所有词向量与x的余弦相似度\n",
    "    cos = torch.matmul(W,x) / (torch.sum(W*W,dim=1) * torch.sum(x*x) + 1e-9).sqrt()\n",
    "    # 数组中前k+1个最大的值的下标\n",
    "    _ , topk = torch.topk(cos, k = k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]: # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 15])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3],[4,5,6]]\n",
    "np.sum(a,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
