{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, in_channels, out_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        net = []\n",
    "        for i in range(num_convs):\n",
    "            in_c = in_channels + i * out_channels \n",
    "            # 越到后面，通道数积累得越来越多，所以谓之稠密\n",
    "            net.append(conv_block(in_c, out_channels)) \n",
    "        self.net = nn.ModuleList(net)\n",
    "        # 计算输出通道数\n",
    "        self.out_channels = in_channels + num_convs * out_channels\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # 在通道维上将输入和输出连结，然后传给后续的网络\n",
    "            # 所谓稠密：通道总数增加\n",
    "            X = torch.cat((X, Y), dim=1)  \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个有2个输出通道数为10的卷积块。\n",
    "\n",
    "使用通道数为3的输入时，\n",
    "\n",
    "我们会得到通道数为3+2×10=3+2×10=23的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 8, 8])\n",
      "torch.Size([4, 23, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# 2个conv_block,\n",
    "# DenseBlock的输入通道：3\n",
    "# conv_block的输出通道：10\n",
    "blk = DenseBlock(2, 3, 10) \n",
    "X = torch.rand(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "print(X.shape)\n",
    "print(Y.shape) \n",
    "# torch.Size([4, 23, 8, 8])\n",
    "# 23 = 3 + 2 * 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增长率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过渡层\n",
    "由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。\n",
    "\n",
    "过渡层用来控制模型复杂度。它**通过1×1卷积层来减小通道数**，并使用步幅为2的平均池\n",
    "化层减半高和宽，从而进一步降低模型复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对上一个例子的稠密块的输出使用通道数为10的过渡层，\n",
    "\n",
    "1.输出通道减少为10，\n",
    "\n",
    "2.宽高均减少一半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23, 10)\n",
    "blk(Y).shape # torch.Size([4, 10, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet首先使用同ResNet一样的单卷积层和最大池化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "# 此时输出通道：64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "接下来DenseNet使用的是4个稠密块DenseBlock："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, growth_rate = 64, 32  # num_channels为当前的通道数，增长率32意味着每个稠密块将增加4x32=128个通道\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4] # 使用4个稠密块，每个稠密块内部使用4个卷积层conv_block\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    DB = DenseBlock(num_convs, num_channels, growth_rate) # growth_rate实际就是指定conv_block的输出通道数\n",
    "    net.add_module(\"DenseBlosk_%d\" % i, DB)\n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels = DB.out_channels\n",
    "    # 在稠密块之间加入通道数减半的过渡层\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        net.add_module(\"transition_block_%d\" % i, transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同ResNet一样，最后接上全局池化层和全连接层来输出:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add_module(\"BN\", nn.Bat`chNorm2d(num_channels))\n",
    "net.add_module(\"relu\", nn.ReLU())\n",
    "net.add_module(\"global_avg_pool\", d2l.GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)\n",
    "net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, 10))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查每个子模块的输出维度，确保网络无误："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "1  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "2  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "3  output shape:\t torch.Size([1, 64, 24, 24])\n",
      "DenseBlosk_0  output shape:\t torch.Size([1, 192, 24, 24])\n",
      "transition_block_0  output shape:\t torch.Size([1, 96, 12, 12])\n",
      "DenseBlosk_1  output shape:\t torch.Size([1, 224, 12, 12])\n",
      "transition_block_1  output shape:\t torch.Size([1, 112, 6, 6])\n",
      "DenseBlosk_2  output shape:\t torch.Size([1, 240, 6, 6])\n",
      "transition_block_2  output shape:\t torch.Size([1, 120, 3, 3])\n",
      "DenseBlosk_3  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "BN  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "relu  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "global_avg_pool  output shape:\t torch.Size([1, 248, 1, 1])\n",
      "fc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1, 1, 96, 96))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
