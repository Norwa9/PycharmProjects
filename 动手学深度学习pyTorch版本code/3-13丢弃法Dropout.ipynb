{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了权重衰减，常使用**丢弃法**来解决过拟合问题。\n",
    "丢弃法有多种，这里特指倒置丢弃法（inverted dropout）\n",
    "\n",
    "当对谋个隐藏层使用丢弃法时，该层的任一隐藏单元将有一定概率被丢弃掉。设丢弃概率为p，那么有p的概率hi会**被清零**，有1-p的概率hi会处于1-p做**拉伸**。\n",
    "\n",
    "丢弃率是丢弃法的超参数。\n",
    "\n",
    " **特点:** \n",
    "丢弃法的特点是不改变其输入的期望值。设随机变量Xi为0和1的概率分别为p和1-p，使用丢弃法时我们计算新的隐藏单元hi‘：\n",
    "$$ h_{i}^{\\prime}=\\frac{X_{i}}{1-p} h_{i} $$\n",
    "由于E(Xi) = 1-p，所以\n",
    "$$E\\left(h_{i}^{\\prime}\\right)=\\frac{E\\left(\\xi_{i}\\right)}{1-p} h_{i}=h_{i}$$\n",
    "\n",
    "\n",
    "**原理：** 在隐藏层中使用丢弃法，若hi被丢弃，则反向传播时，与被丢弃的hi相关的权重梯度均为0。因为任意的隐藏单元都有可能被丢弃，所以输出层的计算无法过度依靠其中的任一个，从而在训练模型时达到正则化的作用，被用来应对过拟合。\n",
    "\n",
    "**注意：** 在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/luowei/PycharmProjects/动手学深度学习pyTorch版本code', '/opt/anaconda3/lib/python38.zip', '/opt/anaconda3/lib/python3.8', '/opt/anaconda3/lib/python3.8/lib-dynload', '', '/opt/anaconda3/lib/python3.8/site-packages', '/opt/anaconda3/lib/python3.8/site-packages/aeosa', '/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions', '/Users/luowei/.ipython', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X,drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1- drop_prob\n",
    "    if keep_prob == 0:\n",
    "        return torch.zero_like(X)\n",
    "    \n",
    "    # > keep_prob的下标对应的元素都被清0\n",
    "    mask = (torch.rand(X.shape) < keep_prob).float() \n",
    "    \n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.0000,  1.2500,  2.5000,  3.7500,  0.0000,  6.2500,  7.5000,  8.7500],\n",
      "        [10.0000, 11.2500, 12.5000, 13.7500, 15.0000, 16.2500, 17.5000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(16).view(2,8)\n",
    "print(dropout(X,0))\n",
    "print(dropout(X,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  定义模型参数\n",
    "将定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0,0.01,size=(num_inputs,num_hiddens1)),dtype = torch.float,requires_grad=True)\n",
    "b1 = torch.zeros(num_hiddens1,requires_grad=True)\n",
    "W2 = torch.tensor(np.random.normal(0,0.01,size=(num_hiddens1,num_hiddens2)),dtype = torch.float,requires_grad=True)\n",
    "b2 = torch.zeros(num_hiddens2,requires_grad=True)\n",
    "W3 = torch.tensor(np.random.normal(0,0.01,size=(num_hiddens2,num_outputs)),dtype = torch.float,requires_grad=True)\n",
    "b3 = torch.zeros(num_outputs,requires_grad=True)\n",
    "\n",
    "params = [W1,b1,W2,b2,W3,b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1=0.2\n",
    "drop_prob2=0.5\n",
    "\n",
    "def net(X,is_training=True):\n",
    "    X = X.view(-1,num_inputs)\n",
    "    H1 = (torch.matmul(X,W1) + b1).relu()\n",
    "    if is_training:\n",
    "        H1 = dropout(H1,drop_prob1) # 在第一层全连接后添加丢弃层\n",
    "    H2 = (torch.matmul(H1,W2) + b2).relu()\n",
    "    if is_training:\n",
    "        H2 = dropout(H2,drop_prob2)\n",
    "    output = (torch.matmul(H2,W3) + b3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要将此函数写回到d2lzh_pytorch，然后重启一下jupyter kernel\n",
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n = 0.0,0 # 分类正确的数量，总数\n",
    "    for X,y in data_iter:\n",
    "        if isinstance(net,torch.nn.Module):\n",
    "            net.eval() # 评估模式，关闭dropout\n",
    "            acc_sum += (net(X).argmax(dim==1) == y).float().sum().item()\n",
    "            net.train() # 改回训练模式\n",
    "        else: # 自定义的模型\n",
    "            if('is_training' in net.__code__.co_varnames):\n",
    "                acc_sum += (net(X,is_training=False).argmax(dim==1) == y).float().item()\n",
    "            else:\n",
    "                acc_sum += (net(X).argmax(dim==1) == y).float().sum().item()\n",
    "        n+=y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0045, train acc 0.552, test acc 0.759\n",
      "epoch 2, loss 0.0023, train acc 0.783, test acc 0.800\n",
      "epoch 3, loss 0.0019, train acc 0.822, test acc 0.830\n",
      "epoch 4, loss 0.0017, train acc 0.840, test acc 0.831\n",
      "epoch 5, loss 0.0016, train acc 0.848, test acc 0.840\n"
     ]
    }
   ],
   "source": [
    "num_epochs,lr,batch_size = 5,100.0,256\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(num_inputs,num_hiddens1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_prob1),\n",
    "    nn.Linear(num_hiddens1,num_hiddens2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_prob2),\n",
    "    nn.Linear(num_hiddens2,10)\n",
    ")\n",
    "for param in net.parameters():\n",
    "    nn.init.normal_(param,mean=0,std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0015, train acc 0.860, test acc 0.851\n",
      "epoch 2, loss 0.0015, train acc 0.861, test acc 0.856\n",
      "epoch 3, loss 0.0014, train acc 0.867, test acc 0.852\n",
      "epoch 4, loss 0.0014, train acc 0.870, test acc 0.853\n",
      "epoch 5, loss 0.0013, train acc 0.873, test acc 0.837\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(),lr = 0.5)\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params=None,lr=None,optimizer=optimizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
