{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "from mnist.loader import MNIST\n",
    "\n",
    "#Data: read MNIST\n",
    "DATA_PATH = './mnist'\n",
    "mn = MNIST(DATA_PATH)\n",
    "mn.gz = True # Enable loading of gzip-ed files\n",
    "train_img, train_label = mn.load_training()\n",
    "test_img, test_label = mn.load_testing()\n",
    "train_X = np.array(train_img)[0:10000]\n",
    "train_Y = np.array(train_label).reshape(-1, 1)\n",
    "test_X = np.array(test_img)\n",
    "test_Y = np.array(test_label).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(len(train_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM loss\n",
    "对于样本i，其损失函数：\n",
    "$$ \\begin{aligned} L_{i} &=\\sum_{j \\neq y_{i}}\\left\\{\\begin{array}{ll}0 & \\text { if } s_{y_{i}} \\geq s_{j}+1 \\\\ s_{j}-s_{y_{i}}+1 & \\text { otherwise }\\end{array}\\right.\\\\ &=\\sum_{j \\neq y_{i}} \\max \\left(0, s_{j}-s_{y_{i}}+1\\right) \\end{aligned}\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_process(X,Y,W,lamda):\n",
    "    dW = np.zeros((784, 10))\n",
    "    loss = 0.0\n",
    "    for i in range(X.shape[0]):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[Y[i]]\n",
    "        for j in range(10):\n",
    "            if j == Y[i]:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + 1\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:,Y[i][0]] += -X[i].T\n",
    "                dW[:,j] += X[i].T\n",
    "    loss = (loss + lamda * np.sum(W*W)) / X.shape[0]\n",
    "    dW = dW / X.shape[0] + 2 * lamda * W\n",
    "    return dW,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100]LOSS:2830.986, ACC:0.177\n",
      "[2/100]LOSS:2776.955, ACC:0.177\n",
      "[3/100]LOSS:2724.139, ACC:0.177\n",
      "[4/100]LOSS:2672.603, ACC:0.177\n",
      "[5/100]LOSS:2622.408, ACC:0.178\n",
      "[6/100]LOSS:2573.428, ACC:0.178\n",
      "[7/100]LOSS:2525.665, ACC:0.177\n",
      "[8/100]LOSS:2479.219, ACC:0.177\n",
      "[9/100]LOSS:2433.979, ACC:0.177\n",
      "[10/100]LOSS:2390.075, ACC:0.177\n",
      "[11/100]LOSS:2347.405, ACC:0.177\n",
      "[12/100]LOSS:2305.902, ACC:0.178\n",
      "[13/100]LOSS:2265.710, ACC:0.178\n",
      "[14/100]LOSS:2226.740, ACC:0.178\n",
      "[15/100]LOSS:2189.048, ACC:0.178\n",
      "[16/100]LOSS:2152.522, ACC:0.179\n",
      "[17/100]LOSS:2117.079, ACC:0.179\n",
      "[18/100]LOSS:2082.841, ACC:0.178\n",
      "[19/100]LOSS:2049.673, ACC:0.178\n",
      "[20/100]LOSS:2017.623, ACC:0.178\n",
      "[21/100]LOSS:1986.723, ACC:0.177\n",
      "[22/100]LOSS:1956.950, ACC:0.178\n",
      "[23/100]LOSS:1928.233, ACC:0.178\n",
      "[24/100]LOSS:1900.485, ACC:0.178\n",
      "[25/100]LOSS:1873.688, ACC:0.179\n",
      "[26/100]LOSS:1847.837, ACC:0.180\n",
      "[27/100]LOSS:1822.908, ACC:0.180\n",
      "[28/100]LOSS:1798.857, ACC:0.181\n",
      "[29/100]LOSS:1775.671, ACC:0.183\n",
      "[30/100]LOSS:1753.347, ACC:0.183\n",
      "[31/100]LOSS:1731.874, ACC:0.184\n",
      "[32/100]LOSS:1711.185, ACC:0.184\n",
      "[33/100]LOSS:1691.236, ACC:0.186\n",
      "[34/100]LOSS:1672.019, ACC:0.186\n",
      "[35/100]LOSS:1653.487, ACC:0.188\n",
      "[36/100]LOSS:1635.622, ACC:0.189\n",
      "[37/100]LOSS:1618.406, ACC:0.189\n",
      "[38/100]LOSS:1601.815, ACC:0.191\n",
      "[39/100]LOSS:1585.762, ACC:0.191\n",
      "[40/100]LOSS:1570.199, ACC:0.192\n",
      "[41/100]LOSS:1555.083, ACC:0.192\n",
      "[42/100]LOSS:1540.406, ACC:0.193\n",
      "[43/100]LOSS:1526.180, ACC:0.194\n",
      "[44/100]LOSS:1512.385, ACC:0.194\n",
      "[45/100]LOSS:1498.972, ACC:0.195\n",
      "[46/100]LOSS:1485.958, ACC:0.196\n",
      "[47/100]LOSS:1473.313, ACC:0.196\n",
      "[48/100]LOSS:1461.019, ACC:0.197\n",
      "[49/100]LOSS:1449.077, ACC:0.197\n",
      "[50/100]LOSS:1437.447, ACC:0.197\n",
      "[51/100]LOSS:1426.106, ACC:0.198\n",
      "[52/100]LOSS:1415.095, ACC:0.199\n",
      "[53/100]LOSS:1404.358, ACC:0.201\n",
      "[54/100]LOSS:1393.879, ACC:0.202\n",
      "[55/100]LOSS:1383.634, ACC:0.202\n",
      "[56/100]LOSS:1373.609, ACC:0.203\n",
      "[57/100]LOSS:1363.815, ACC:0.204\n",
      "[58/100]LOSS:1354.234, ACC:0.206\n",
      "[59/100]LOSS:1344.885, ACC:0.207\n",
      "[60/100]LOSS:1335.750, ACC:0.209\n",
      "[61/100]LOSS:1326.790, ACC:0.209\n",
      "[62/100]LOSS:1318.003, ACC:0.210\n",
      "[63/100]LOSS:1309.392, ACC:0.212\n",
      "[64/100]LOSS:1300.934, ACC:0.213\n",
      "[65/100]LOSS:1292.656, ACC:0.214\n",
      "[66/100]LOSS:1284.536, ACC:0.214\n",
      "[67/100]LOSS:1276.561, ACC:0.215\n",
      "[68/100]LOSS:1268.719, ACC:0.216\n",
      "[69/100]LOSS:1261.000, ACC:0.217\n",
      "[70/100]LOSS:1253.393, ACC:0.218\n",
      "[71/100]LOSS:1245.904, ACC:0.218\n",
      "[72/100]LOSS:1238.531, ACC:0.219\n",
      "[73/100]LOSS:1231.275, ACC:0.221\n",
      "[74/100]LOSS:1224.123, ACC:0.221\n",
      "[75/100]LOSS:1217.083, ACC:0.222\n",
      "[76/100]LOSS:1210.141, ACC:0.223\n",
      "[77/100]LOSS:1203.291, ACC:0.224\n",
      "[78/100]LOSS:1196.524, ACC:0.225\n",
      "[79/100]LOSS:1189.858, ACC:0.225\n",
      "[80/100]LOSS:1183.293, ACC:0.227\n",
      "[81/100]LOSS:1176.817, ACC:0.227\n",
      "[82/100]LOSS:1170.417, ACC:0.229\n",
      "[83/100]LOSS:1164.094, ACC:0.230\n",
      "[84/100]LOSS:1157.852, ACC:0.230\n",
      "[85/100]LOSS:1151.691, ACC:0.232\n",
      "[86/100]LOSS:1145.596, ACC:0.232\n",
      "[87/100]LOSS:1139.578, ACC:0.233\n",
      "[88/100]LOSS:1133.634, ACC:0.234\n",
      "[89/100]LOSS:1127.755, ACC:0.235\n",
      "[90/100]LOSS:1121.934, ACC:0.236\n",
      "[91/100]LOSS:1116.170, ACC:0.237\n",
      "[92/100]LOSS:1110.473, ACC:0.238\n",
      "[93/100]LOSS:1104.843, ACC:0.238\n",
      "[94/100]LOSS:1099.274, ACC:0.240\n",
      "[95/100]LOSS:1093.764, ACC:0.240\n",
      "[96/100]LOSS:1088.315, ACC:0.241\n",
      "[97/100]LOSS:1082.913, ACC:0.242\n",
      "[98/100]LOSS:1077.568, ACC:0.243\n",
      "[99/100]LOSS:1072.277, ACC:0.244\n",
      "[100/100]LOSS:1067.037, ACC:0.245\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "lr = 0.00001\n",
    "epochs = 100\n",
    "\n",
    "#Model\n",
    "W = np.random.rand(784, 10)\n",
    "lamda = 0.95\n",
    "\n",
    "for step in range(epochs):\n",
    "    dW, loss = svm_loss_process(train_X,train_Y,W,lamda)\n",
    "    W = W - lr * dW\n",
    "    test = test_X.dot(W)\n",
    "    correct_num = np.sum(np.argmax(test,axis=1) == test_Y.flatten())\n",
    "    acc = correct_num / train_X.shape[0]\n",
    "    print(\"[%d/%d]LOSS:%.3f, ACC:%.3f\" %(step+1, epochs, loss, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
